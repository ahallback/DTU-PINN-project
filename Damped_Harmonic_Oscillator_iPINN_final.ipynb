{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eh-xvrQbTW0p"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Damped Harmonic Oscillator - the inverse problem\n",
    "### Karen, Andreas and Clara\n",
    "\n",
    "Often in industrial applications of PINNs, the parameters of the governing PDEs are unknown. This is also known as **the inverse problem**. Within the framework of PINN, inverse problems are solved by expanding the PINN to also estimate the PDE-parameters. In practice, we add the PDE-parameters to the list of parameters to be optimized. \n",
    "\n",
    "To accommodate that the underlying PDE-parameters can be quite large, we estimate not the parameters directly but how much they differ from a \"best first guess\". Say we estimate $\\mu$ and a aposteori guess that $\\mu=\\bar{\\mu}$, then we estimate $d_{\\mu}$, such that the estimated parameter $\\hat{\\mu}$ is given by\n",
    "$$\n",
    "\\hat{\\mu} = \\bar{\\mu} + d_{\\mu} \\ .\n",
    "$$\n",
    "\n",
    "The ODE, boundary and initial conditions and loss terms are as in the Damped Harmonic Oscillator PINN notebook, except the fact that we vary $\\delta$ and $\\omega_0$ and sample new training point for each set of ODE-parameters. \n",
    "\n",
    "## Training a PINN\n",
    "Again we have chosen to start training a PINN with a fully connected neural network which has 4 hidden layers and 64 neurons in each of the hidden layers with $\\mathrm{tanh}$ as activation function.\n",
    "\n",
    "### Sources\n",
    "This notebook is inspired by https://github.com/benmoseley/harmonic-oscillator-pinn/blob/main/Harmonic%20oscillator%20PINN.ipynb and\n",
    "https://github.com/benmoseley/DLSC-2023/blob/main/lecture-5/PINN%20demo.ipynb and https://towardsdatascience.com/inverse-physics-informed-neural-net-3b636efeb37e\n",
    "\n",
    "### Analytical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oscillator(d, w0, x):\n",
    "    \"\"\"Defines the analytical solution to the 1D underdamped harmonic oscillator problem. \n",
    "    Equations taken from: https://beltoforion.de/en/harmonic_oscillator/\"\"\"\n",
    "    assert d < w0\n",
    "    w = np.sqrt(w0**2-d**2)\n",
    "    phi = np.arctan(-d/w)\n",
    "    A = 1/(2*np.cos(phi))\n",
    "    cos = torch.cos(phi+w*x)\n",
    "    sin = torch.sin(phi+w*x)\n",
    "    exp = torch.exp(-d*x)\n",
    "    y  = exp*2*A*cos\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = [2, 4, 8]\n",
    "omegas = [20, np.sqrt(800), 40]\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "par_train = []\n",
    "\n",
    "for d in deltas:\n",
    "    for w0 in omegas:\n",
    "\n",
    "        # get the analytical solution over the full domain\n",
    "        x = torch.linspace(0,1,500).view(-1,1)\n",
    "        y = oscillator(d, w0, x).view(-1,1)\n",
    "        # print(x.shape, y.shape)\n",
    "\n",
    "        # slice out a small number of points from the LHS of the domain\n",
    "        x_data = x[0:200:5]\n",
    "        y_data = y[0:200:5]\n",
    "        # print(x_data.shape, y_data.shape)\n",
    "\n",
    "        # append training data\n",
    "        x_train.append(x_data.flatten())\n",
    "        y_train.append(y_data.flatten())\n",
    "        par_train.append(torch.Tensor([d, w0]))\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.title(f'$\\delta$ = {d}, $\\omega_0$ = {np.round(w0,3)}')\n",
    "        # plt.plot(x, y, color=\"tab:orange\", label=\"Exact solution\")\n",
    "        # plt.scatter(x_data, y_data, color=\"tab:orange\", label=\"Training data\")\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "\n",
    "# convert lists to tensors-arrays\n",
    "x_train = torch.stack(x_train)\n",
    "y_train = torch.stack(y_train)\n",
    "par_train = torch.stack(par_train)\n",
    "# print(x_train.shape, y_train.shape, par_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "r9mHXSjamxdn"
   },
   "outputs": [],
   "source": [
    "# define network architecture\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "  def __init__(self, n_inputs, n_hidden, n_outputs):\n",
    "    super().__init__()\n",
    "    self.input = nn.Sequential(\n",
    "      nn.Linear(n_inputs, n_hidden),\n",
    "      nn.Tanh(),\n",
    "    )\n",
    "    \n",
    "    self.hidden = nn.Sequential(\n",
    "      nn.Linear(n_hidden, n_hidden),\n",
    "      nn.Tanh(),\n",
    "      nn.Linear(n_hidden, n_hidden),\n",
    "      nn.Tanh(),\n",
    "    )\n",
    "\n",
    "    self.output = nn.Sequential(\n",
    "    nn.Linear(n_hidden, n_hidden),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(n_hidden, n_outputs),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.input(x)\n",
    "    x = self.hidden(x)\n",
    "    x = self.output(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gif_PIL(outfile, files, fps=5, loop=0):\n",
    "    \"Helper function for saving GIFs\"\n",
    "    imgs = [Image.open(file) for file in files]\n",
    "    imgs[0].save(fp=outfile, format='GIF', append_images=imgs[1:], save_all=True, duration=int(1000/fps), loop=loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Qg58rYa69P9F",
    "outputId": "33aedf8e-c4e2-4cd6-c9e5-61585f3e8eff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = 1, mu = 4.0, omega = 20.0\n",
      "predicted mu = 3.994738084729761, percentage mu = -0.1315474510192871, predicted omega = 19.97441577911377, percentage omega = -0.12792587280273438\n",
      "m = 1, mu = 4.0, omega = 28.284271240234375\n",
      "predicted mu = 5.520895004272461, percentage mu = 38.022377014160156, predicted omega = 28.191622734069824, percentage omega = -0.3275585472583771\n",
      "m = 1, mu = 4.0, omega = 40.0\n",
      "predicted mu = 8.059544086456299, percentage mu = 101.48858642578125, predicted omega = 39.89581775665283, percentage omega = -0.26045799255371094\n",
      "m = 1, mu = 8.0, omega = 20.0\n",
      "predicted mu = 7.970769882202148, percentage mu = -0.36537647247314453, predicted omega = 19.957429885864258, percentage omega = -0.21285055577754974\n",
      "m = 1, mu = 8.0, omega = 28.284271240234375\n",
      "predicted mu = 8.462034702301025, percentage mu = 5.77542781829834, predicted omega = 28.211446523666382, percentage omega = -0.2574734091758728\n",
      "m = 1, mu = 8.0, omega = 40.0\n",
      "predicted mu = 11.737208843231201, percentage mu = 46.715118408203125, predicted omega = 39.88692283630371, percentage omega = -0.2826881408691406\n",
      "m = 1, mu = 16.0, omega = 20.0\n",
      "predicted mu = 15.762348175048828, percentage mu = -1.4853239059448242, predicted omega = 19.837493896484375, percentage omega = -0.812530517578125\n",
      "m = 1, mu = 16.0, omega = 28.284271240234375\n",
      "predicted mu = 15.697874069213867, percentage mu = -1.88828706741333, predicted omega = 27.929873943328857, percentage omega = -1.2529820203781128\n",
      "m = 1, mu = 16.0, omega = 40.0\n",
      "predicted mu = 18.113497734069824, percentage mu = 13.209366798400879, predicted omega = 39.21374034881592, percentage omega = -1.9656468629837036\n"
     ]
    }
   ],
   "source": [
    "physics_xs = torch.linspace(0, 1, 40).view(-1, 1).requires_grad_(True)\n",
    "\n",
    "m_value = 1\n",
    "\n",
    "for i, [d, w0] in enumerate(par_train):\n",
    "  print(f'm = {m_value}, mu = {d*2}, omega = {w0}')\n",
    "    \n",
    "  data_x = x_train[i]\n",
    "  data_x = data_x[:, None]\n",
    "  data_y = y_train[i]\n",
    "  data_y = data_y[:, None]\n",
    "\n",
    "  d_mu = torch.nn.Parameter(torch.zeros(1, requires_grad=True)) # initialize d_mu with value 0\n",
    "  mu_bar = torch.nn.Parameter(torch.ones(1, requires_grad=True)*4) # initialize mu_bar with value 4\n",
    "  d_omega = torch.nn.Parameter(torch.zeros(1, requires_grad=True)) # initialize d_omega with value 0\n",
    "  omega_bar = torch.nn.Parameter(torch.ones(1, requires_grad=True)*30) # initialize mu_bar with value 30\n",
    "\n",
    "  torch.manual_seed(123)\n",
    "  model = FullyConnected(1, 64, 1)\n",
    "  optimizer2 = torch.optim.Adam(list(model.parameters())+ [d_mu] + [d_omega], lr=1e-3)#, weight_decay=1e-4)\n",
    "  scheduler = StepLR(optimizer2, step_size=10000, gamma=0.9)\n",
    "\n",
    "  files = []\n",
    "  mus = []\n",
    "  ws = []\n",
    "  for i in range(60001):\n",
    "    optimizer2.zero_grad()\n",
    "\n",
    "    # data loss\n",
    "    pred_y = model(data_x)\n",
    "    data_loss = torch.mean((pred_y - data_y) ** 2) # Mean squared error\n",
    "\n",
    "    # physics loss\n",
    "    physics_pred_y = model(physics_xs)\n",
    "    dx = torch.autograd.grad(physics_pred_y, physics_xs, torch.ones_like(physics_pred_y), create_graph=True)[0]\n",
    "    dx2 = torch.autograd.grad(dx, physics_xs, torch.ones_like(dx), create_graph=True)[0]\n",
    "    residual = dx2 + (mu_bar + d_mu) * dx + ((omega_bar + d_omega) ** 2) * physics_pred_y\n",
    "    physics_loss = torch.mean(residual**2)\n",
    "\n",
    "    # boundary conditions loss\n",
    "    t_boundary = torch.zeros(1, requires_grad=True) # torch.tensor([0])\n",
    "    x_boundary = torch.ones(1, requires_grad=True) #torch.tensor([1])\n",
    "    # dx_boundary = torch.zeros(1, requires_grad=True) #torch.tensor([0])\n",
    "    x0_predicted = model(t_boundary)\n",
    "    dx_predicted = torch.autograd.grad(x0_predicted, t_boundary, torch.ones_like(t_boundary), create_graph=True)[0]\n",
    "    residual_x0 = x0_predicted - x_boundary\n",
    "    residual_dx = dx_predicted\n",
    "    boundary_loss = torch.mean(residual_x0 ** 2) \n",
    "    initial_loss = torch.mean(residual_dx ** 2)\n",
    "\n",
    "    loss = (1e-4) * physics_loss + data_loss + boundary_loss + initial_loss\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # record parameter values\n",
    "    mus.append(mu_bar.item() + d_mu.item())\n",
    "    ws.append(omega_bar.item() + d_omega.item())\n",
    "\n",
    "    if i % 2000 == 0 and i != 0:\n",
    "      detached_t = physics_xs.detach()\n",
    "\n",
    "      # get the analytical and predicted solution over the full domain\n",
    "      x = torch.linspace(0,1,500).view(-1,1)\n",
    "      y = oscillator(d, w0, x).view(-1,1)\n",
    "      pred_ys = model(x).detach()\n",
    "\n",
    "      plt.figure(figsize=(12,2.5))\n",
    "\n",
    "      plt.subplot(1,4,1)\n",
    "      plt.plot(x, pred_ys, color=\"tab:blue\", label=\"Prediction\")\n",
    "      plt.plot(x, y, color=\"tab:orange\", label=\"Exact solution\")\n",
    "      # plt.scatter(detached_t, torch.zeros_like(detached_t))\n",
    "      plt.scatter(data_x, data_y, color=\"tab:orange\", label=\"Training data\")\n",
    "      plt.title(f'Predictions after step {i}')\n",
    "      plt.ylim(-0.75, 1.1)\n",
    "      plt.xlabel('t')\n",
    "      plt.ylabel('x')\n",
    "      plt.legend()\n",
    "\n",
    "      plt.subplot(1,3,2)\n",
    "      plt.title(\"$\\mu$\")\n",
    "      plt.plot(mus, label=\"PINN estimate $\\mu$\", color=\"tab:green\")\n",
    "      plt.hlines(2*d, 0, 100004, label=\"True value\", color=\"tab:grey\")\n",
    "      plt.xlabel(\"Training step\")\n",
    "      plt.xlim(0, 100005)\n",
    "      plt.ylim(3, 11)\n",
    "      plt.legend()\n",
    "\n",
    "      plt.subplot(1,3,3)\n",
    "      plt.title(\"$\\omega_0$\")\n",
    "      plt.plot(ws, label=\"PINN estimate $\\omega_0$\", color=\"tab:green\")\n",
    "      plt.hlines(w0 , 0, 100004, label=\"True value\", color=\"tab:grey\")\n",
    "      plt.xlabel(\"Training step\")\n",
    "      plt.xlim(0, 100005)\n",
    "      plt.ylim(18, 30)\n",
    "      plt.legend()\n",
    "\n",
    "      file = \"plots/ipinn_%.8i.png\"%(i)\n",
    "      plt.savefig(file, bbox_inches='tight', pad_inches=0.1, dpi=100, facecolor=\"white\")\n",
    "      files.append(file)\n",
    "      \n",
    "      plt.show()\n",
    "\n",
    "  save_gif_PIL(f'DHO_ipinn_{d}_{np.round(w0,2)}.gif', files, fps=20, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
